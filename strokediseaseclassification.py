# -*- coding: utf-8 -*-
"""PredictiveAnalyticsSubmission_RifqiNovandi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bu_yo_WmvtTR4wZkN3QpEFUxRpYajMng

# Stroke Disease Classification
*by: [Rifqi Novandi](https://github.com/rifqinvnd)*

## Background
In this machine learning project, the overall topic that will be resolved is in the field of stroke health, where it will try to predict the possibility of a stroke in a person with certain conditions based on several factors including: age, certain diseases (hypertension, heart disease), smoking, etc.

## 1. Install and import required library
"""

# install the newest scikit-learn library

!pip install -U scikit-learn

# library for prepare the dataset
import os
import zipfile

# library for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# library for data processing
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

# library for modeling
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV, StratifiedKFold

# library for model evaluation
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

"""## 2. Data preparation
### 2.1 Prepare Kaggle username and key
"""

# menyiapkan kredensial environment Kaggle

os.environ['KAGGLE_USERNAME'] = 'rifqinovandi'
os.environ['KAGGLE_KEY'] = '61655b112a6218032cc7743aab07e371'

"""### 2.2 Download and preprocess the dataset"""

# Download the dataset with Kaggle CLI
!kaggle datasets download -d fedesoriano/stroke-prediction-dataset

# Extract zip file to CWD
files = "/content/stroke-prediction-dataset.zip"
zip = zipfile.ZipFile(files, 'r')
zip.extractall('/content')
zip.close()

"""## 3. Data Understanding
### 3.1 Read data with pandas
"""

df = pd.read_csv('/content/healthcare-dataset-stroke-data.csv')
df.head()

"""### 3.2 Explore Dataset information"""

# check dataset info
df.info()

# check dataset shape
df.shape

# check missing value of the data
df.isna().sum()

# describe numeric column
df.describe()

"""### 3.3 Data Visualization
#### 3.3.1 Check dataset target
"""

df['stroke'].value_counts().plot(kind='bar')

"""#### 3.3.2 Check datatype of the column"""

cats = list(df.select_dtypes(include=['object','bool']) )
nums = list(df.select_dtypes(include=['int64','float64']))
print(cats)
print(nums)

# classify data for the encoding
encoder = []
onehot = []

for col in cats:
   if len(df[col].unique()) == 2:
        encoder.append(col)
   else:
        onehot.append(col)

print(encoder)
print(onehot)

df_labencoded = df.copy()

for col in encoder:
    df_labencoded[col] = df_labencoded[col].astype('category').cat.codes
df_labencoded.head()

# check feature correlation to the target
df_labencoded.corr().round(2)

# visualize feature correlation to the target
for col in onehot:
    df_loop = df_labencoded[[col,'stroke']].copy()
    onehots = pd.get_dummies(df_loop[col], prefix=col)
    df_loop = df_loop.join(onehots)
    plt.figure(figsize=(15, 8))
    print(sns.heatmap(df_loop.corr(), cmap='Blues', annot=True, fmt='.2f'))

"""## 4. Data Preparation
### 4.1 Handling missing values with mean subtitution
"""

df['bmi'].fillna(df['bmi'].mean(), inplace=True)

# recheck missing value
df.isna().sum()

# check duplicated data
duplicate = df.duplicated()
df[duplicate].sum()

"""### 4.2 Remove unnecessary column"""

df = df.drop(['id'], axis=1)
nums.remove('id')
df.head()

"""### 4.3 Remove outliers of the dataset"""

plt.figure(figsize=(15, 7))
for i in range(0, len(nums)):
    plt.subplot(2, 3, i+1)
    sns.boxplot(y=df[nums[i]],color='green',orient='v')
    plt.tight_layout()

outlier = ['avg_glucose_level', 'bmi']

Q1 = df[outlier].quantile(0.25)
Q3 = df[outlier].quantile(0.75)
IQR = Q3 - Q1
df = df[~((df[outlier]<(Q1-1.5*IQR))|(df[outlier]>(Q3+1.5*IQR))).any(axis=1)]
df.reset_index(drop=True)

"""### 4.4 Convert Categorical column to Numerical"""

# onehot encoding for categorical feature
df = pd.get_dummies(df)
df.head()

"""### 4.5 Pre Modeling Steps"""

# separate feature and target
X = df.drop(columns = ['stroke'])
y = df['stroke']

# using SMOTE Techniqe
sm = SMOTE(random_state=111)
X_sm , y_sm = sm.fit_resample(X,y)

print(f'''Shape of X before SMOTE:{X.shape}
Shape of X after SMOTE:{X_sm.shape}''',"\n\n")

print(f'''Target Class distributuion before SMOTE:\n{y.value_counts(normalize=True)}
Target Class distributuion after SMOTE :\n{y_sm.value_counts(normalize=True)}''')

"""### 4.7 Split train data and test data"""

X_train, X_test, y_train, y_test = train_test_split(
    X_sm,
    y_sm,
    test_size = .2,
    random_state = 111)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

"""### 4.8 Normalize data with StandardScaler"""

X_train = StandardScaler().fit_transform(X_train)
X_test = StandardScaler().fit_transform(X_test)

"""## 5. Machine Learning Modeling

### 5.1 Using K-Nearest-Neighbors-Classifier
"""

# Create simple model
baseline_model = KNeighborsClassifier()
baseline_model.fit(X_train, y_train)

# Test model with test data
y_pred = baseline_model.predict(X_test)

# Simple model report
acc = accuracy_score(y_test, y_pred)
print('Testing-set Accuracy score is:', acc)
print('Training-set Accuracy score is:',accuracy_score(y_train,baseline_model.predict(X_train)))

baseline_report = classification_report(y_test, y_pred, output_dict=True, target_names=['No Stroke', 'Stroke'])
pd.DataFrame(baseline_report).transpose()

# Confussion matrix
baseline_cf = confusion_matrix(y_test, y_pred)
sns.heatmap(baseline_cf, annot = True, fmt = "d")

"""### 5.2 Develop the Machine Learning model with Hyperparameter Tuning HalvingGridSearchCV"""

# set hyperparameter
param_grid = {'n_neighbors': [1, 2],
              'p': [1, 2],
              'weights': ["uniform","distance"],
              'algorithm':["ball_tree", "kd_tree", "brute"],
              }

# Seek for the best hyperparameter with HalvingGridSearchCV
new_param = HalvingGridSearchCV(baseline_model, 
                                param_grid, 
                                cv=StratifiedKFold(n_splits=3, random_state= 123, shuffle=True),
                                resource='leaf_size',
                                max_resources=20,
                                scoring='recall',
                                aggressive_elimination=False).fit(X_train, y_train)

# Result of the hyperparameter tuning
print(f"Best Hyperparameter {new_param.best_estimator_} with score {new_param.best_score_}")

# Set the model with the best hyperparameter
model = KNeighborsClassifier(algorithm='ball_tree', leaf_size=18, n_neighbors=1, p=1, weights='distance')
model.fit(X_train, y_train)

# Test tuned model with test data
y_pred = model.predict(X_test)

# Tuned model report
acc = accuracy_score(y_test, y_pred)
print('Testing-set Accuracy score is:', acc)
print('Training-set Accuracy score is:',accuracy_score(y_train,model.predict(X_train)))

improvement_report = classification_report(y_test, y_pred, output_dict=True, target_names=['No Stroke', 'Stroke'])
pd.DataFrame(improvement_report).transpose()

"""## 6. Model Evaluation
### 6.1 Matrix comparison between the initial baseline model and the hyperparameter-tuned model
"""

metrics = pd.DataFrame({'accuracy' : [baseline_report['accuracy'], improvement_report['accuracy']],
                        'f1-score_0' : [baseline_report['No Stroke']['f1-score'],improvement_report['No Stroke']['f1-score']],
                        'precision_0' : [baseline_report['No Stroke']['precision'],improvement_report['No Stroke']['precision']],
                        'recall_0' : [baseline_report['No Stroke']['recall'],improvement_report['No Stroke']['recall']],
                        'f1-score_1' : [baseline_report['Stroke']['f1-score'],improvement_report['Stroke']['f1-score']],
                        'precision_1' : [baseline_report['Stroke']['precision'],improvement_report['Stroke']['precision']],
                        'recall_1' : [baseline_report['Stroke']['recall'],improvement_report['Stroke']['recall']]},
                        index=['Model Baseline','Tuned Model'])
multiheader = [('','accuracy'),
               ('No Stroke', 'f1-score'),
               ('No Stroke', 'precision'),
               ('No Stroke', 'recall'),
               ('Stroke', 'f1-score'),
               ('Stroke', 'precision'),
               ('Stroke', 'recall')]
metrics.columns = pd.MultiIndex.from_tuples(multiheader)
metrics

"""## Closing
Machine Learning Model to predict stroke in humans can be used because it reaches a recall score of 97.7% which is where this model can predict stroke very well although it still needs to be improved even up to a recall score of 100%.

### References
- Scikit-learn Docummentation: [https://scikit-learn.org/stable/modules/classes.html](https://scikit-learn.org/stable/modules/classes.html)
- Report References: [https://github.com/fahmij8/ML-Exercise/blob/main/MLT-1/MLT_Proyek_Submission_1.ipynb](https://github.com/fahmij8/ML-Exercise/blob/main/MLT-1/MLT_Proyek_Submission_1.ipynb)
- Project: [https://www.kaggle.com/muhamilham/supervised-learning-stroke-prediction](https://www.kaggle.com/muhamilham/supervised-learning-stroke-prediction)
"""
